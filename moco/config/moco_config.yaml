# Wandb settings
log_wandb: True #False
wandb_project_name: "MoCoV2_BP"
wandb_name: "ConvNeXt-Base + 4 Augs"
wandb_notes: "ConvNeXt-Base,Fatcat, Amp_Seg-0.95sec, Augmentation: Seq Perturb + Reverse 0.5 -> Time mask -> Pitch shift -> Time stretch"


# Dataset settings
data_dir: "/mnt/gestalt/home/ddmanddman/beatport_analyze/chorus_audio_16000_095sec_npy_bass_other_new"
seg_dir: "/mnt/gestalt/home/buffett/beatport_analyze/chorus_audio_16000_095sec_npy_bass_other_new/amp_08"  # path to dataset (required)
model_dict_save_dir: "/mnt/gestalt/home/buffett/moco_model_dict/bass_other_new_amp08"
lmdb_dir: "/mnt/gestalt/home/buffett/beatport_analyze/lmdb"
batch_size: 256 # 256  # mini-batch size
workers: 16  # number of data loading workers
sample_rate: 16000
n_fft: 1024
window_size: 1024
hop_length: 160
n_mels: 64
fmin: 20
fmax: 8000
prefetch_factor: 4
persistent_workers: true
SEGMENT_TIME: 0.95
SAMPLE_RATE: 16000
AMP_THRES: 0.5
fixed_second: 0.3
num_seq_segments: 5
p_ts: 0.5 # 0.5 # Sequence Perturbation
p_ps: 0.5 # 0.4 # Pitch Shift
p_tm: 0.5 # 0.5 # Time Mask
p_tstr: 0.5 # 0.5 # Time Stretch
tm_min_band_part: 0.05 #0.1
tm_max_band_part: 0.1 #0.15
tm_fade: True
p_mask: 0.5 # 0.5 # Time Mask
semitone_range: [-2, 2] # [-4, 4]
tstr_min_rate: 0.8
tstr_max_rate: 1.25
sp_method: "fixed" # "random", "fixed", "adaptive", "reverse"
norm_stats: [-5.36902,  3.7100384]
amp_name: "_amp08"


# Model architecture settings
arch: "ConvNeXt" #"resnet50"  # model architecture
convnext_model: "base"
model_names: ["ConvNeXt", "resnet50"]  # list of available model names
channels: 1


# Training settings
epochs: 200 # 800 # number of total epochs to run
start_epoch: 0  # manual epoch number (useful on restarts)
lr: 0.03  # initial learning rate
schedule: [120, 160]  # learning rate schedule (when to drop lr by 10x)
momentum: 0.9  # momentum of SGD solver
weight_decay: 1.0e-4  # weight decay
print_freq: 10  # print frequency

# Checkpoint settings
resume: ""  # path to latest checkpoint

# Distributed training settings
world_size: 1  # number of nodes for distributed training
rank: 0 # node rank for distributed training
dist_url: "tcp://localhost:10001" #"tcp://224.66.41.62:23456"  # url used to set up distributed training
dist_backend: "nccl"  # distributed backend
gpu: [0,1] #[0,1,2,3]  # GPU id to use
multiprocessing_distributed: true  # Use multi-processing distributed training

# Random seed
seed: null # 42  # seed for initializing training

# MoCo specific settings; To run MoCo v2, set --mlp --moco-t 0.2 --aug-plus --cos.
moco_dim: 128  # feature dimension
moco_K: 65536  # queue size; number of negative keys
moco_m: 0.999  # moco momentum of updating key encoder
moco_T: 0.2 # MoCoV2 -> 0.2, MoCoV1 -> 0.07  # softmax temperature
moco_mlp: true # MoCoV2 -> true, MoCoV1 -> false # use mlp head
moco_aug_plus: true # MoCoV2 -> true, MoCoV1 -> false  # use moco v2 data augmentation
moco_cos: true # MoCoV2 -> true, MoCoV1 -> false  # use cosine lr schedule 