{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buffett/miniconda3/envs/py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          TrainingArguments, Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Device ‚ûú cuda\n"
     ]
    }
   ],
   "source": [
    "# ============ 1. Âü∫Êú¨ÂèÉÊï∏ ============\n",
    "RANDOM_SEED = 42\n",
    "TEST_SIZE   = 0.2\n",
    "BATCH_SIZE  = 32\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Embedding Device ‚ûú {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ 2. ËºâÂÖ•Ë≥áÊñô ============\n",
    "train_path = Path(\"review_data.csv\")\n",
    "test_path  = Path(\"X_test.csv\")\n",
    "\n",
    "# review_data.csv: id | review | helpfulness\n",
    "df_train = pd.read_csv(train_path, header=0, names=[\"id\", \"review\", \"helpfulness\"])\n",
    "X_text, y = df_train[\"review\"].tolist(), df_train[\"helpfulness\"].values\n",
    "\n",
    "# Ê∏¨Ë©¶ÈõÜÂè™Êúâ id„ÄÅreview\n",
    "df_test = pd.read_csv(test_path, header=0, names=[\"id\", \"review\"])\n",
    "X_test_text = df_test[\"review\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ 3. Split ============\n",
    "X_train_text, X_val_text, y_train, y_val = train_test_split(\n",
    "    X_text,\n",
    "    y,\n",
    "    test_size     = TEST_SIZE,\n",
    "    random_state  = RANDOM_SEED,\n",
    "    stratify      = y,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetForSequenceClassification were not initialized from the model checkpoint at microsoft/mpnet-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2118/2118 [00:00<00:00, 8038.81 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 530/530 [00:00<00:00, 6914.81 examples/s]\n",
      "/home/buffett/miniconda3/envs/py310/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-14 08:25:35,424] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buffett/miniconda3/envs/py310/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/buffett/miniconda3/envs/py310/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlvsym'\n",
      "/home/buffett/miniconda3/envs/py310/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlopen'\n",
      "/home/buffett/miniconda3/envs/py310/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlclose'\n",
      "/home/buffett/miniconda3/envs/py310/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlerror'\n",
      "/home/buffett/miniconda3/envs/py310/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlsym'\n",
      "collect2: error: ld returned 1 exit status\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mphilip910323\u001b[0m (\u001b[33mbuffett0323\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/buffett/research/query_ss/hw_0509/wandb/run-20250514_082536-logn4lyb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/buffett0323/huggingface/runs/logn4lyb' target=\"_blank\">mpnet-helpfulness</a></strong> to <a href='https://wandb.ai/buffett0323/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/buffett0323/huggingface' target=\"_blank\">https://wandb.ai/buffett0323/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/buffett0323/huggingface/runs/logn4lyb' target=\"_blank\">https://wandb.ai/buffett0323/huggingface/runs/logn4lyb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='335' max='335' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [335/335 01:07, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.562200</td>\n",
       "      <td>0.508502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.418000</td>\n",
       "      <td>0.479742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.346700</td>\n",
       "      <td>0.428628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.291400</td>\n",
       "      <td>0.455634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.245100</td>\n",
       "      <td>0.450391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=335, training_loss=0.3726846040184818, metrics={'train_runtime': 69.8001, 'train_samples_per_second': 151.719, 'train_steps_per_second': 4.799, 'total_flos': 1393173038131200.0, 'train_loss': 0.3726846040184818, 'epoch': 5.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============ 4. ËºâÂÖ•Ê®°Âûã ============\n",
    "model_name = \"microsoft/mpnet-base\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(model_name)#.to(DEVICE)\n",
    "model      = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(DEVICE)\n",
    "\n",
    "def tok(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "train_ds = Dataset.from_dict({\"text\": X_train_text, \"label\": y_train}).map(tok, batched=True)\n",
    "val_ds   = Dataset.from_dict({\"text\": X_val_text,   \"label\": y_val  }).map(tok, batched=True)\n",
    "\n",
    "# ============ 5. TrainingArguments ============\n",
    "args = TrainingArguments(\n",
    "    output_dir                  = \"mpnet-helpfulness\",\n",
    "    fp16                        = torch.cuda.is_available(),\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    per_device_eval_batch_size  = BATCH_SIZE,\n",
    "    evaluation_strategy         = \"epoch\",\n",
    "    save_strategy               = \"epoch\",\n",
    "    save_total_limit            = 1,\n",
    "    num_train_epochs            = 5,\n",
    "    learning_rate               = 2e-5,\n",
    "    weight_decay                = 0.01,\n",
    "    seed                        = RANDOM_SEED,\n",
    "    load_best_model_at_end      = True,\n",
    "    metric_for_best_model       = \"eval_loss\",\n",
    "    greater_is_better           = False,\n",
    "    logging_strategy            = \"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(model, args, train_dataset=train_ds, eval_dataset=val_ds)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/662 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 662/662 [00:00<00:00, 5148.80 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úî Â∑≤Ëº∏Âá∫ mpnet_finetune_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# ============ 4. Êé®Ë´ñ‰∏¶Â≠ò CSV ============\n",
    "# 4-1. Ê∫ñÂÇô test dataset\n",
    "test_ds = Dataset.from_dict({\"text\": X_test_text}).map(tok, batched=True)\n",
    "\n",
    "# 4-2. È†êÊ∏¨Ôºõtrainer.predict ÊúÉÂõûÂÇ≥ logits\n",
    "pred_logits = trainer.predict(test_ds).predictions\n",
    "pred_labels = np.argmax(pred_logits, axis=1)\n",
    "\n",
    "# 4-3. Ëº∏Âá∫\n",
    "df_submit = pd.DataFrame({\n",
    "    \"Id\": df_test[\"id\"],\n",
    "    \"helpfulness\": pred_labels\n",
    "})\n",
    "df_submit.to_csv(\"mpnet_finetune_predictions.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"‚úî Â∑≤Ëº∏Âá∫ mpnet_finetune_predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
