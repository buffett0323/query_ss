# Pre-process
segment_second: 8 # half in 2 seconds
piece_second: 4
log_wandb: True #False
wandb_project_name: "SimSiam Training"
wandb_name: "SwinT+4/ALL+256*8win+Crop+Noise+TS+TW+FM+TM"
wandb_notes: "SSBP SwinT Training Fatcat server"


# distributed training
data_dir: "/mnt/gestalt/home/ddmanddman/beatport_analyze/chorus_audio_16000_npy"
verse_data_dir: "/mnt/gestalt/home/ddmanddman/beatport_analyze/verse_audio_16000_npy"
model_dict_save_dir: "/mnt/gestalt/home/buffett/simsiam_model_dict/swint_model_dict_0313"
resume_training_path: "/mnt/gestalt/home/buffett/simsiam_model_dict/swint_model_dict_0313/checkpoint_20.ckpt"
# data_dir: "/home/buffett/NAS_NTU/beatport_analyze/chorus_audio_16000_npy"
# verse_data_dir: "/home/buffett/NAS_NTU/beatport_analyze/verse_audio_16000_npy"
# model_dict_save_dir: "/home/buffett/NAS_NTU/timbre_encoder_model_dict/swint/"
world_size: 1 #-1
rank: 0 # -1
dist_url: 'tcp://localhost:10001' #'tcp://224.66.41.62:23456'
dist_backend: 'nccl'
multiprocessing_distributed: True #True
find_unused_parameters: True # False
melspec_transform: False #Wavegram will transform for us
data_augmentation: True
random_slice: True # Random slice enhance model's ability of capturing timbre
workers: 48 # 24 for terra, 56 for fatcat, 104 for sinica
gpu: [1] #[0,1,2,3]


# train options
img_mean: -1.100174903869629
img_std: 14.353998184204102
seed: 42 # sacred handles automatic seeding when passed in the config
batch_size: 256 #256 #512 
gradient_accumulation_steps: 8 # Effective Batch Size = Batch Size per GPU × Number of GPUs × Gradient Accumulation Steps
start_epoch: 0
epochs: 200 # 100
check_val_every_n_epoch: 4
early_stop_patience: 5 #10
lr: 0.05 #0.001 # 0.03  # 0.0003
momentum: 0.9
warmup_epochs: 10 #30 #10
dim: 2048
pred_dim: 512
fix_pred_lr: True
print_freq: 20


# dataset options
max_length: 64
n_fft: 1024 #1024 #2048
hop_length: 256 #188 #320 #512 #1024
sample_rate: 16000 #44100 Temporarily transform to 16000
window_size: 1024 #512
n_mels: 256 #256 #128
img_size: 256 #256
swint_window_size: 8 #8
fmin: 0 #20
fmax: 8000 # 16000/2
pin_memory: True
drop_last: True #False
persistent_workers: True


# model options
encoder_name: "SwinTransformer" #"Wavegram_Logmel128_Cnn14"
channels: 1


# loss options
optimizer: "Adam" # or LARS (experimental)
weight_decay: 1.0e-4 #1.0e-4 #1.0e-6 # "optimized using LARS [...] and weight decay of 10−6"
temperature: 0.1 #0.5 # see appendix B.7.: Optimal temperature under different batch sizes
lr_decay_factor: 0.05 #0.01


# reload options
model_path: "save" # set to the directory containing `checkpoint_##.tar` 
reload: False
resume: '' # path to latest checkpoint (default: none)

# knn options
knn_k: 3 #200
knn_t: 1.0 #0.2
