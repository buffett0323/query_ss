# Training Configuration
batch_size: 64 #128
num_workers: 16

# Data loading optimizations
prefetch_factor: 8  # Number of batches to prefetch per worker
use_memory_mapping: true  # Use memory mapping for large files
drop_last_train: false     # Drop incomplete batches during training

# Paths
data_dir: "/mnt/gestalt/home/buffett/rendered_adsr_unpaired_mel_npy" #"/home/buffett/dataset/rendered_adsr_unpaired_mel_npy"
# h5_path: "/mnt/gestalt/home/buffett/rendered_adsr_unpaired_mel_h5/adsr_mel.h5" #"/home/buffett/dataset/rendered_adsr_unpaired_mel_h5/adsr_mel.h5"

# Wandb
wandb_use: false #true
wandb_dir: "/mnt/gestalt/home/buffett/dataset/adsr_reg_logs" #"/home/buffett/dataset/adsr_reg_logs"
wandb_project: "adsr-reg"
wandb_name: "adsr_mel_625_ntu"

# Model
d_model: 1024
d_out: 4  # A,D,S,R
n_heads: 8
n_layers: 16
patch_size: 16
patch_stride: 10
input_channels: 1
spec_shape: [128, 256]

# Optimizer
lr: 1.0e-4
epochs: 400
save_interval: 20


# FFT parameters
sr: 44100
n_fft: 2048
win_length: 2048
hop_length: 512
n_mels: 128
fmin: 20
fmax: 22050
mel_mean: -12.619391
mel_std: 5.2448645

# Lightning specific settings
accelerator: "gpu"
devices: [2]  # Use GPU device 3 specifically
precision: "16-mixed"  # Fixed precision setting
gradient_clip_val: 1.0
accumulate_grad_batches: 1
log_every_n_steps: 50
